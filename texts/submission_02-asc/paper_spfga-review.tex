\documentclass[review,preprint,authoryear,12pt]{elsarticle}

%
%	LINE NUMBERING FOR REVISION
%
\usepackage{lineno}
%

%
%	FILE ENCODING
%
\usepackage[utf8]{inputenc}
%

%
%	PAGE GEOMETRY
%
%\usepackage{geometry}
%\geometry{a4paper}
%

%
%	AMS STUFF
%
\usepackage{amsfonts}
%\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
%

%
%	ACRONYMS
%
\usepackage{acronym}
%
\acrodef{GA}{Genetic algorithms}
\acrodef{rmse}[error]{root-mean-square error}
\acrodef{GAPoly}[\textsc{GAPoly}]{Genetic Algorithms for Polynomials}
\acrodef{EPR}{Evolutionary Polynomial Regression}
\acrodef{EPRR}[EPR$^2$]{Evolutionary Polynomial Regression with Regularisation}
%

%
%	FIGURE AND STUFF ENVIRONMENT
%
\usepackage{graphicx}
%

%
%	ALGORITHMS PACKAGES
%
\usepackage{algpseudocode}
\usepackage{algorithm}
%

%
%	HYPERREFERENCES
%
\usepackage{hyperref}
%


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%
%	DOCUMENT COMMANDS
%
\newcommand{\sample}[1]{\ensuremath{^{\left(#1\right)}}}
\newcommand{\note}[1]{\textbf{[~#1~]}}
%

%
%
%%% BEGIN DOCUMENT
%
%
\begin{document}
\begin{frontmatter}

\title{A Method for Regularisation of Evolutionary Polynomial Regressions using Support Vector Machines}

\author[ue,labmag]{Francisco ~Coelho\corref{fc}}
\ead{fc@di.uevora.pt}
\cortext[fc]{Corresponding author}

\author[fcul,labmag]{João ~Pedro ~Neto}
\ead{jpn@di.fc.ul.pt}


\address[ue]{Dept. Informática, Universidade de Évora, Rua Romão Ramalho 58, 7000-671 Évora}
\address[fcul]{Dept. Informática, Faculdade de Ciências da Universidade de Lisboa, Campo Grande 1749-016 Lisboa}
\address[labmag]{Laboratory of Agent Modelling (LabMAg)}
%
%\date{[{\sc draft} \today]}
%
\begin{abstract}
While many applications require models that have no acceptable linear approximation the simpler nonlinear models are defined by polynomials. The use of genetic algorithms to find polynomial models, \ac{EPR}, provides an automatic method to build such models from data but still poses challenges due to the complexity of the search and different definitions of optimal solution.

This paper describes \ac{EPRR}, an algorithm that extends the general \ac{EPR} method with regularisation and a set of experiences to compare both flavours of \ac{EPR} and other usual regression methods including Random Forests, Linear Regression and Support Vector Machines on common datasets. The empiric conclusion of those experiments is that \ac{EPRR} achieves better fitting errors and that, with respect to the plain \ac{EPR}, computation time is shorter.
\end{abstract}
%
\begin{keyword}
evolutionary polynomial regression \sep regularisation \sep feature extraction \sep dimensionality reduction 
\end{keyword}
\end{frontmatter}
%
%	Line numbers for revision
%
%
\linenumbers
%
\section{Introduction}

With notable exceptions (\emph{e.g.} neural networks) machine learning regression techniques are based on linear models. The linearity assumption has many advantages including reduced computational complexity and strong theoretical framework. However nonlinearity is unavoidable in many application scenarios, specially those with phase transitions or feedback loops, so common in engineering, ecology, cybernetics and other areas. The kernel trick in Support Vector Machines alleviates this problem by allowing a non-linear transformation of  the feature-space. \note{Find references.}

Polynomials, one of the most studied subjects in mathematics, generalise li\-ne\-ar functions and define, perhaps, the simplest and most used nonlinear models. Applications include colorimetric calibration \citep{mendes2005adaptive}, explicit formul\ae\ for turbulent pipe flows \citep{davidson1999method}, computational linguistics \citep{sanchez2009obtaining} and more recently, analytical techniques for cultural heritage materials \citep{csefalvayova2010use}, liquid epoxy moulding process \citep{chan2011modeling}, B-spline surface reconstruction \citep{galvez2012iterative}, product design \citep{chan2012development} or forecasting cyanotoxins presence in water reservoirs \citep{garcia2013hybrid}. These examples not only illustrate the wide spectrum of applications but, additionally, work in each one uses, at some point, a genetic algorithm.

\ac{GA} where, arguably, one of the hottest topics of research in the recent decades and with good reason since they outline an optimisation scheme easy to conceptualise and with very broad application. If a nonlinear (or otherwise) model requires parameterization \acp{GA} provide a simple and often effective approach to search for locally optimal parameters. Research related to genetic algorithms abound and spans from the 1950s seminal work of Nils Aall Barricelli \citep{barricelli1962numerical} in the Institute for Advanced Study of Princeton to today's principal area of study for thousands of researchers, covered in hundreds of conferences, workshops and other meetings. Perhaps the key impulse to \acp{GA} come from John Holland's work and his book ``Adaptation in Natural and Artificial Systems'' \citep{holland1975adaptation}.  

One interesting take on genetic algorithms, named \emph{genetic programming} by John Koza \citep{koza1992genetic}, proposed the use of \acp{GA} to search the syntactic structure of complex functions. This syntactic structure search is keen to the central ideas of deep learning \citep{bengio2013representation, bengio2009learning}, a subarea of machine learning actually producing quite promising results (\emph{e.g.} in \cite{Tarlow:2013fk}). It is also related to the work presented in this paper in the sense that, unlike linear models that have a simple structure, $y=\sum_i \beta_i x_i$, nonlinear (in particular polynomial) models pose an additional ``structure" search problem.

The idea of using \acp{GA} to find a polynomial regression is not new \citep{maertens2006genetic, yu2008optimal, wu2009novel} but still generates original research \citep{hofwing2011optimal, cetisli2011polynomial}. \note{Insert references and resume of EPR; Redo the purpose of this work.} In line with that research this work describes a general method to find a polynomial regression of a given dataset. The optimal regression minimizes a cost function that accounts for both the \ac{rmse} and a regularization factor to avoid over-fitting. 

%A method that produces \emph{adequate} models from observed complex data has many uses. For example by a scientist to better understand the source of the data or by an autonomous agents adapting to the environment.  

It turns out that, discarding the computational cost of training, the polynomial regression method presented here, \ac{GAPoly}, provides a quite competitive regression method. Indeed, it is only systematically out-performed by random forests, an \emph{ensemble} method.

The remainder of this paper is organized as usual: the next section describes the details of our method and is followed by a presentation of some performance results. The last section draws some conclusions and points future research tasks.


\section{Genetic Algorithms for Polynomials}

This section is dedicated to the description of an algorithm to find a polynomial regression from a given dataset. It starts with a brief introduction and outline of the algorithm and proceeds into core details as the encoding used to represent individual polynomial instances in the \ac{GA} populations and the regularization of the cost function.

An usual representation of polynomials is
$$
p\left( x_1, \ldots, x_m\right) = \sum_i \theta_i q_i
$$
where each $q_i$ is a monomial, $q_i = \prod_{j} x_j^{\alpha_{ij}}$, the exponents are non-negative integers, $\alpha_{ij}\in\mathbb{N}_0$, and the coefficients are real valued, $\theta_i \in \mathbb{R}$. For example $p\left( x_1, x_2, x_3 \right) = 2 x_1 + x_2 x_3 + \frac{1}{2} x_1^2 x_3$ has monomials $q_1 = x_1, q_2 = x_2 x_3$ and $q_3 = x_1^2 x_3$, coefficients $\theta_1 = 2, \theta_2 = 1$ and $\theta_3 = 1/2$ and exponents $\alpha_{1,1} = 1, \alpha_{2,2}=1, \alpha_{2,3} = 1, \alpha_{3,1} = 2, \alpha_{3,3} = 1$ and all other $\alpha_{ij} = 0$. The exponents alone are a matrix that defines the monomial structure of the polynomial, $A = \left[\alpha_{ij}\right]$.

For the example above the matrix of monomials is
$$
A = \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\
2 & 0 & 1
\end{array}
\right] \sim 
\left[ 
\begin{array}{ccc}
x_1 \\
x_2 x_3 \\
x_1^2 x_3
\end{array}
\right]
$$
where each row defines a monomial and each column represents a variable. Changing the order of the rows doesn't change the polynomial whereas changing the order of the columns corresponds to changing the respective variables.
%Notice that if we interpret matrix $A$ as a single column of monomials and set the coefficients $T = \left[ \begin{array}{ccc} 2 & 1 & \frac{1}{2} \end{array} \right ]$ then
%$$
%p\left( x_1, x_2, x_3 \right) = TA = \left[ \begin{array}{ccc} 2 & 1 & \frac{1}{2} \end{array} \right ]\left[ 
%\begin{array}{ccc}
%x_1 \\
%x_2 x_3 \\
%x_1^2 x_3
%\end{array}
%\right].
%$$

This representation of polynomials makes the problem of structure search very clear: except for the trivial cases, the number of possible monomials given $n$ variables and a maximum joint degree $d$ grows exponentially with either $n$ or $d$. But more importantly, the polynomial regression problem can be naturally split into two subproblems:
%
\begin{enumerate}
\item For a given set of monomials $\mathcal{P} = \left\lbrace q_1, \ldots, q_k\right\rbrace$, find the regression coefficients $\theta_1,\ldots,\theta_k$ that minimize the \ac{rmse} on a given dataset;

\item Find the fittest set of monomials, \emph{i.e.} the polynomial that minimizes the \ac{rmse} on the same dataset;
\end{enumerate}
%
More precisely, concerning the first problem, let $\mathcal{D}$ be a dataset with $n$ observations of variables $Y, X_1,\ldots,X_m$ and $\mathcal{P} = \left\lbrace q_1,\ldots, q_k\right\rbrace$ a set of $k$ monomial expressions over $X_1,\ldots,X_m$. Define the hypothesis\footnote{The expression $q|_{X=x}$ reads ``\emph{replace all instances of $X$ by $x$ in $q$}''.}
%
\begin{eqnarray*}
h_{\Theta,\mathcal{P}}\left(x_1,\ldots,x_m\right) &=& \sum_{j = 1}^k \theta_j q_j|_{X_i=x_i,\forall 1 \leq i \leq m}
\end{eqnarray*}
%
and let the cost
%
\begin{eqnarray}
J_{\textrm{fit}}\left(\Theta;\mathcal{P},\mathcal{D}\right)  & =  \nonumber \\
\sqrt{\frac{1}{n}\sum_{i=1}^n \left( y\sample{i} - h_{\Theta,\mathcal{P}}\left( x_1\sample{i},\ldots,x_m\sample{i} \right) \right)^2 }\label{eq:rmse}
\end{eqnarray}
%
be the usual \acf{rmse} function. Now the first problem can be stated as:
%
\emph{Given a dataset $\mathcal{D}$ and a set of monomials $\mathcal{P}$, find parameters $\Theta$ that minimize $J_{\textrm{fit}}\left(\Theta;\mathcal{P},\mathcal{D}\right)$.}

%
It turns out that this problem can be solved as a usual linear regression problem by expanding $\mathcal{D}$ with columns that replicate the monomials in $\mathcal{M}$.

The second problem is treated in the \ac{GA} setting: Let $\mathcal{D}$ be a dataset as above and $Q$ a set of polynomials. For each polynomial $p\in Q$ let $\mathcal{P}_p$ be the set of monomials in $p$ (without the coefficients) and compute the fitness
%
\begin{eqnarray*}
\phi_p &=& \min_\Theta J_{\textrm{fit}}\left(\Theta;\mathcal{P}_p,\mathcal{D}\right)
\end{eqnarray*}
%
by solving the first problem. With a fitness of every instance, a \ac{GA} will apply genetic operators (usually mutation and crossover) to evolve the population $Q$ until a reasonable approximation of a local minimum is found. 
%
\begin{algorithm}[tb]
\begin{algorithmic}
\Function{GAPoly}{$D,pop_0,\epsilon$}
	\State $pop \gets pop_0;\: err \gets 1.0+\epsilon$
	\While{$err > \epsilon$}
		\State $pop \gets \Call{IterateGA}{pop}$
		\State $pop \gets \Call{Sort}{pop, key = J}$\Comment{Sort population by regression error}
		\State $err \gets J\left(\Call{First}{pop}\right)$
	\EndWhile
	\State\Return $\Call{First}{pop}$
\EndFunction
\end{algorithmic}
\caption{\ac{GAPoly} uses linear regression to find monomial coefficients that minimize the \ac{rmse} over a dataset and \acp{GA} to explore the space of polynomials. The role of the linear regression step is to produce a fitting error that sorts the population. At exit the \ac{rmse} of the fittest instance is bounded by $\epsilon$.}\label{alg:gapoly}
\end{algorithm} 
%
Notice that the properties of \acp{GA} and linear regression entail that
the composition of \acp{GA} with linear regression, as defined in Algorithm \ref{alg:gapoly}, converges to a polynomial that is a local minimum of the fitness function, encapsulated in the \ac{rmse} function $J_{\textrm{fit}}$.
%

%
Subsection \ref{subs:polynomial.encoding} describes the encoding of individual polynomial instances as chromosomes and other parameters of the utilized \ac{GA} implementation. The regularization of the cost function is discussed in subsection \ref{subs:cost.function}.

%
\subsection{Polynomial Encoding}\label{subs:polynomial.encoding}
%
The encoding for any polynomial will be as follows:
\begin{enumerate}
\item an initial segment detailing which monomials are active (the first monomial is always active), this is represented in unary description, i.e., each monomial is identified by a single bit
\item the remaining bits are split into $k$ sets of equal size, each one representing a monomial
\item each monomial is split into $m$ sets of $d$ size each, \emph{i.e.}, a variable
\item for each variable, the remaining bits are the binary description of the variable degree, \emph{i.e.}, the maximum exponent is given by $2^d-1$
\end{enumerate}

Let's see an example: consider polynomial $x_1^3 x_3 + x_3^7 + x_1 x_2$ with $k = 4, m = 3$ and $d=3$. One possible encoding would be:
\begin{center}
\texttt{110 - 011,000,001 ; 000,000,111 ; 001,001,000 ; 110,010,101}
\end{center}


(for reading purposes the semicolons separate monomials, the commas separate variables)

The first three bits inform that the second and third monomials are active while the fourth is not (as said, the first monomial is always active). This last monomial does not enter neither in the polynomial regression nor in the \ac{rmse} (fitness) evaluation. However, it acts as a kind of junk DNA, becoming active when, in a future mutation or crossover, the third bit of the entire sequence flips from~0 to~1.

Let's interpret the first monomial description, \verb!011,000,001!. It is divided by three since $m=3$. The first triple \verb!011! is the binary description of the exponent of variable $x_1$ which is 3, so the first monomial includes $x_1^3$. The second triple, \verb!000!, means that $x_2$ is not part of the monomial. The third triple \verb!001! says that variable $x_3$ has exponent $1$. The first monomial is~$x_1^3 x_3$.

Notice that all binary descriptions give rise to valid polynomials. However this is not a bijective mapping. For each polynomial there are multiple representations. For example, $x_1+x_2$ and $x_2+x_1$ have different representations. The authors considered that more complex mappings in order to force a one to one mapping would impact negatively in the algorithm's performance.

% If the coding consists of entirely zeros, by convention, it describes polynomial $x_1$. This has to do with the execution of the polynomial regression that would fail if we interpret it as the zero polynomial. Anyway, for progressive larger  binary descriptions, the chances of getting this zero description decrease exponentially, so it does not impact in any meaningful way in the algorithm's performance.

\subsection{Cost Function}\label{subs:cost.function}

The polynomial regression \ac{rmse} considered so far is based on the ability to predict the test set after the polynomial regression has found the appropriate coefficients $\theta_i$ for each one of the monomials $q_i$.

This \ac{rmse} function tends to prefer more complex polynomials, namely in the number of monomials which provides the regression algorithm for more fitting possibilities. One way to balance this is to provide a regularization term into the \ac{rmse} function. Our proposal is to include a multiplicative factor proportional to the number of monomials. Thus, the error from equation \ref{eq:rmse} defines
\begin{eqnarray}
J_{\textrm{reg}}\left(\Theta;\mathcal{P},\mathcal{D}\right) &=& \lambda^{k} J_{\textrm{fit}}\left(\Theta;\mathcal{P},\mathcal{D}\right)\label{eq:rmse-reg}
\end{eqnarray}
%
where $k$ is the number of monomials in the polynomial. When $\lambda > 1$ polynomials with more monomials are penalized.

Somewhat unexpectedly after some experiences it was found that lower values for $\lambda$ provide better, even if marginal, results. Figure~\ref{Abalone_dataset_lambdas} shows regularization results for the Abalone and Auto MPG datasets with ten runs for each $\lambda$. The following section  includes information about these datasets.

%
\begin{figure*}[tb]\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{figure_1a.pdf}
%
% RIGHT PANE
&
%
%
\includegraphics[width=0.49\textwidth]{figure_1b.pdf}
\end{tabular}

\caption{Error distribution by regularization exponent for the Abalone and Auto MPG datasets. The box plots summarize the error values of 10 runs for each value of $\lambda$. The smallest overall error is shown in gray and, in both cases, was achieved with $\lambda=0.8$.}
\label{Abalone_dataset_lambdas}\label{Auto-MPG_dataset_lambdas}

\end{center}\end{figure*}

The typical inflection point lies around $\lambda = 0.8$. The dataset results for applying the proposed regression method use the regularization parameter with this value.

\subsection{Genetic Operators}

To perform the genetic algorithm it was used the R package genalg~\citep{willighage2012genalg}. The operators were the standard ones: (a)~crossover, \emph{i.e.}, a pair of solutions from the previous generations are combined by splitting and mixing their respective representations, and (b)~mutation, changing the values of single bits; the mutation chance applied in the datasets was 5\%. There was also elitism between generations, \emph{i.e.}, 20\% of the best solutions survive to the next generation.

\section{Experimental Results}

The results were found using \texttt{R} programming language~\citep{R}\footnote{The datasets and the \texttt{R} code used to produce the results and plots in this paper are available online at \url{https://github.com/jpneto/GenAlgPoly}.}. To compare this paper's proposed algorithm we applied the exact same train and test samples using several well-known learning algorithms for regression, namely: Linear Regression, Support Vector Machines~\citep{Meyer12}, Regression Trees~\citep{Therneau13}, Conditional Inference Trees~\citep{Hothorn06, Strobl07, Strobl08} and Random Forests~\citep{Liaw02}

In order to train and test the performance of \ac{GAPoly} several mainstream datasets were used. For each dataset, we selected 70\% for training purposes and the remaining observations to make a test set in order to compute the estimated \ac{rmse}. To achieve more robust results, each dataset were processed $25$~times, each one with different samples for the train and test sets. For the datasets with attribute values of different magnitudes, a preliminary scaling was executed. The results below are box plots for the test set error predictions over these different runs.

\begin{figure*}[tb]\begin{center}
\includegraphics[width=0.98\textwidth]{figure_2.pdf}

\caption{Results for Artificial dataset. As shown, \ac{GAPoly} was able to find the exact polynomial structure that generates the dataset, reducing the test set prediction error to zero.  The regression methods depicted are: 1. \ac{GAPoly}, 2. Random Forests, 3. Linear Regression, 4. SVM, 5. Regression Trees and 6. Conditional Inference Trees}
\label{artificial_dataset1_lambda1.0}

\end{center}\end{figure*}

\begin{figure*}[tb]\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{figure_3a.pdf}
%
&
%
\includegraphics[width=0.48\textwidth]{figure_3b.pdf}
%
\\
%
\includegraphics[width=0.48\textwidth]{figure_3c.pdf}
%
&
%
\includegraphics[width=0.48\textwidth]{figure_3d.pdf}
%
\end{tabular}

\caption{Summary results for different regression methods on the Housing, Abalone, Auto MPG and Kinematics datasets. Random Forests, an ensemble method (``2'' in the plots), has the better results. However \ac{GAPoly} achieves similar errors in two of the datasets. For the remaining two \ac{GAPoly} produces similar errors to the other classic regression methods.  The regression methods depicted in these figures are: 1. \ac{GAPoly}, 2. Random Forests, 3. Linear Regression, 4. SVM, 5. Regression Trees and 6. Conditional Inference Trees}
\label{Housing_dataset_lambda0.8_25runs}
\label{Abalone_dataset_lambda0.8_25runs}
\label{Auto-Mpg_dataset_lambda0.8_25runs}
\label{Kinematics300_lambda0.8_25runs}
\end{center}\end{figure*}

\begin{description}
\item{\textsc{Artificial}} this is an artificial dataset with four numeric features, $x_1, \ldots x_4$, where $x_1,x_3$ are outcomes from Poisson random variables, and $x_2,x_4$ from Normal random variables. The dependent variable~$y$ is given by expression $x_2x_4^2 + x_1^2x_3 + 5$. The dataset includes $n=50$~observations.

This dataset was used in order to verify if \ac{GAPoly} was able to find the polynomial relation, which the algorithm did (cf. figure~\ref{artificial_dataset1_lambda1.0}). The genetic algorithm run with a population of $n=100$~solutions with $50$~iterations for each run.

In the next datasets, the population of the genetic algorithm had size~$n=250$ with 100~iterations for each run.

\item{\textsc{Housing}}: This data set concerns the task of predicting housing values in areas of Boston. There are $m=13$ continuous attributes and the dependent variable is the median value of owner-occupied homes in \$1000's. There are $n=506$ observations.

Just as an example of the model the \ac{GAPoly} algorithm outputs: in this dataset the best polynomial,

$$y = -0.12 x_6 x_9^4 + 0.78 x_6 - 0.4 x_9^2 x_{13} + 0.17 x_{13}^2 - 0.044$$

where the attributes mean:
$x_6$, RM average number of rooms per dwelling;
$x_9$, RAD index of accessibility to radial highways;
$x_{13}$, Lower status of the population.

For comparison, if we access the mean decrease in accuracy found by Random Forests, the most important attributes are --- in decreasing order --- $x_{13}, x_6, x_5$ (but $x_5$ is already considered $4$ times less important than $x_6$). Both algorithms agree in two of their three most important attributes.

\item{\textsc{Abalone}} This dataset can be used to predict the age of a abalone shell using the given $m=8$ numeric attributes concerning several physical measurements. There are $n=4177$ observations. 

% best polynomial, rsme 0.65
% $$y = 0.52 s_1^2 x_5 - 0.13 x_2 x_8 -0.75 x_1^2 x_6 + 0.78 x_8 + 0.18 $$

\item{\textsc{Auto MPG}} This dataset is used to predict fuel consumption in miles per gallon, based on two discrete and five continuous attributes ($m=7$). There are $n=398$ observations.

% best polynomial, \ac{rmse} 0.32
%$$ y = 0.4 x_6 + 0.02 x_5^3 x_6 - 0.85 x_4 + 0.23 x_4^2 - 0.26 $$

\item{\textsc{Kinematics}} This dataset is concerned with the realistic simulation of the forward kinematics of an 8 link robot arm. The task is to predict the distance of the end-effector from a target using $m=8$ continuous attributes. There are $n=8192$ observations. 

% best polynomial, \ac{rmse} 0.74
% $$ y = 0.24 x_5 - 0.23 x_5 x_6 x_7 - 0.19 x_4 x_7 - 0.5 x_3 $$

\end{description}

\subsection{Convergence speed}

The \ac{GA} quickly proceeds in the first~50 to~100 generations to reasonable error rates. Then, it proceeds slower achieving best solutions with marginal error reduction. Since the entire learning process takes some time, in the current R implementation, placing a limit between 50 to 100 generations already achieves good results, relative to higher iteration values. Figure~\ref{Abalone_fitnessProgress} shows a typical error evolution for the dataset Abalone given two different values for $\lambda$.

\begin{figure*}[tb]
\begin{center}
\includegraphics[width=0.98\textwidth]{figure_4.pdf}
\caption{Error progress for Abalone dataset during a single execution of the genetic algorithm. The figure shows the fitness evolution for two different regularization values. The population for both consisted of $200$ polynomials. The error values seem to stabilize around iteration~250.}
\label{Abalone_fitnessProgress}

\end{center}
\end{figure*}

\section{Conclusion and Future Work}

Of all the non-ensemble regression methods considered, the proposed method is the one that shows, with one exception, the lowest error. Only Random Forest outperforms \ac{GAPoly} systematically (it also outperforms all the other regression algorithms). The single non-ensemble exception --- besides the artificial dataset that uses a straightforward polynomial relation --- is the Auto MPG dataset where \ac{GAPoly} has comparable results. This is evidence that applying standard genetic operators for polynomial model searching is a viable tool for regression purposes.

For complexity considerations \ac{GAPoly} demands some processing time. On a current quad-core computer, processing the Kinematics dataset (with 8k~observations) takes approximately 5~minutes. The processing time can probably be speeded by one to two orders in magnitude if the process is implemented in a low level programming language like \texttt{C++}. However, speed optimization was not the focus of this article.

A cross-validation procedure can be implemented to refine the appropriate parameter values to achieve better errors. Namely, the regularization parameter, $\lambda$, can be tested with several values, instead of being fixed at $0.8$. Other parameters like mutation chance or the amount of elitism could also be tested. However, these type of tests need a low-level, fast implementation of \ac{GAPoly}.

\section*{Acknowledgements}

The authors are grateful to the Fundação para a Ciência e Tecnologia (FCT) and the  R\&D laboratory LabMAg for the financial support given to this work, under the strategic project \textsc{PEst-OE/EEI/UI0434/2011}.

The datasets used herein were selected from Luís Torgo's data  repository, \url{http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}. Most come originally from UCI ML repository, \url{http://archive.ics.uci.edu/ml/}.

The authors wish to thank professor André Falcão for motivation and useful discussions around the article.

%\section{Bibliography}

\bibliographystyle{model2-names}
\bibliography{fullbib}
    
\end{document}

