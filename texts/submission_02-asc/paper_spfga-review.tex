%\documentclass[review,preprint,authoryear,12pt]{elsarticle}
\documentclass[preprint,authoryear,12pt]{elsarticle}

%
%	LINE NUMBERING FOR REVISION
%
%\usepackage{lineno}
%

%
%	FILE ENCODING
%
\usepackage[utf8]{inputenc}
%

%
%	PAGE GEOMETRY
%
%\usepackage{geometry}
%\geometry{a4paper}
%

%
%	AMS STUFF
%
\usepackage{amsfonts}
%\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
%

%
%	ACRONYMS
%
\usepackage{acronym}
%
\acrodef{GA}{Genetic algorithms}
\acrodef{rmse}[error]{root-mean-square}
%\acrodef{EPR}[\textsc{EPR}]{Genetic Algorithms for Polynomials}
%\acrodef{EPRR}[\textsc{EPR.reg}]{Genetic Algorithms for Polynomials with Regularization}

\acrodef{EPR}[EPR]{Evolutionary Polynomial Regression}
\acrodef{EPRR}[EPR$^2$]{Evolutionary Polynomial Regression with Regularization}

\acrodef{EPR}{Evolutionary Polynomial Regression}
\acrodef{EPRR}[EPR$^2$]{Evolutionary Polynomial Regression with Regularization}

\acrodef{SVM}[SVM]{Support Vector Machines}
%

%
%	FIGURE AND STUFF ENVIRONMENT
%
\usepackage{graphicx}
%

%
%	ALGORITHMS PACKAGES
%
\usepackage{algpseudocode}
\usepackage{algorithm}
%

%
%	HYPERREFERENCES
%
\usepackage{hyperref}
%


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%
%	DOCUMENT COMMANDS
%
\newcommand{\sample}[1]{\ensuremath{^{\left(#1\right)}}}
\newcommand{\at}[1]{\ensuremath{\!\left(#1\right)}}
\newcommand{\note}[1]{\textbf{\small [Note\footnote{\textbf{Note:} #1}]}}
\newcommand{\nth}{\ensuremath{^{th}}}
%

%
%
%%% BEGIN DOCUMENT
%
%
\begin{document}
\begin{frontmatter}

\title{A Method for Regularization of Evolutionary Polynomial Regressions}

\author[ue,labmag]{Francisco ~Coelho\corref{fc}}
\ead{fc@di.uevora.pt}
\cortext[fc]{Corresponding author}

\author[fcul,labmag]{João ~Pedro ~Neto}
\ead{jpn@di.fc.ul.pt}


\address[ue]{Dept. Informática, Universidade de Évora, Rua Romão Ramalho 58, 7000-671 Évora}
\address[fcul]{Dept. Informática, Faculdade de Ciências da Universidade de Lisboa, Campo Grande 1749-016 Lisboa}
\address[labmag]{Laboratory of Agent Modelling (LabMAg)}
%
%\date{[{\sc draft} \today]}
%
\begin{abstract}
While many applications require models that have no acceptable linear approximation, the simpler nonlinear models are defined by polynomials. The use of genetic algorithms to find polynomial models from data is known as \ac{EPR}.
%
This paper introduces \ac{EPRR}, an algorithm that extends the \ac{EPR} method and describes a set of experiences on common datasets that compare both flavors of \ac{EPR} and other methods including Linear Regression, Regression Trees and Support Vector Regression.

The empiric conclusion of those experiments is that \ac{EPRR} is able to achieve better fitting than other non-ensemble methods and it has shorter computation time than plain \ac{EPR}.
\end{abstract}
%
\begin{keyword}
evolutionary polynomial regression \sep regularization \sep feature extraction \sep dimensionality reduction 
\end{keyword}
\end{frontmatter}
%
%	Line numbers for revision
%
%
%\linenumbers
%
\section{Introduction}

With notable exceptions (\emph{e.g.} neural networks) machine learning regression techniques produce linear models. The linearity assumption has many advantages including reduced computational complexity and strong the\-o\-re\-ti\-cal framework. However nonlinearity is unavoidable in many application scenarios, specially those with phase transitions or feedback loops, so common in engineering, ecology, cybernetics and other areas. The kernel trick in \ac{SVM} (\cite{scholkopf1997kernel, liang2012eigen, Bao:2013aa}) alleviates this problem by allowing special non-linear transformations of the feature-space. The condition such transformations must meet is known as the \emph{kernel trick}, $k\at{x,x'} = \left< \varphi\at{x}, \varphi\at{x'} \right>$, where $\varphi$ is the feature-space transformation and $\left<\cdot,\cdot\right>$ denotes inner product. The ``trick'' consists on computing the kernel $k\at{x,x'}$ while avoiding the computation of the inner product and the transformations $\varphi\at{x}, \varphi\at{x'}$. A special case of polynomial transformation, the \emph{polynomial kernel}, $k\at{x,x'} = \left<x,x'\right>^d$ is commonly used in regression and classification tasks with \acp{SVM}. However general polynomial transformations do not verify the kernel trick.

Polynomials, one of the most studied subjects in mathematics, generalize li\-ne\-ar functions and define, perhaps, the simplest and most used nonlinear models. Applications include colorimetric calibration \citep{Mendes:2005aa}, explicit formul\ae\ for turbulent pipe flows
 \citep{Davidson:1999aa}, computational linguistics \citep{Sanchez:2009aa} and more recently analytical techniques for cultural heritage materials \citep{Csefalvayova:2010aa}, liquid epoxy moulding process \citep{Chan:2011aa}, B-spline surface reconstruction \citep{Galvez:2012aa}, product design \citep{Chan:2012aa} or forecasting cyanotoxins presence in water reservoirs \citep{Garcia-Nieto:2013aa}. These examples not only illustrate the wide spectrum of applications but, additionally, in each one uses, at some point, a \ac{GA}.

\acp{GA} were, arguably, one of the hottest topics of research in the recent decades and with good reason since they outline an optimization scheme easy to conceptualize and with very broad application. If a nonlinear (or otherwise) model requires parameterization, \acp{GA} provide a simple and often effective approach to search for locally optimal parameters. Research related to genetic algorithms abound and spans from the 1950s seminal work of Nils Aall Barricelli \citep{Barricelli:1962aa} in the Institute for Advanced Study of Princeton to today's principal area of study for thousands of researchers, covered in hundreds of conferences, workshops and other meetings. Perhaps the key impulse to \acp{GA} come from John Holland's work and his book ``Adaptation in Natural and Artificial Systems'' \citep{Holland:1975aa}. 

One interesting variation of genetic algorithms, named \emph{genetic programming} by John Koza \citep{Koza:1992aa}, proposes the use of \acp{GA} to search the syntactic structure of complex functions. Syntactic structure search is also keen to the central ideas of deep learning \citep{Bengio:2009aa,Bengio:2013aa}, a subarea of machine learning actually producing quite promising results (\emph{e.g.} in \cite{Tarlow:2013fk}). It is also related to the work presented in this paper in the sense that, unlike linear models that have a simple structure, $y=\sum_i \beta_i x_i$, nonlinear (in particular polynomial) models pose an additional structure search problem.

The idea of using \acp{GA} to find a polynomial regression is not new \citep{Maertens:2006aa, Yu:2008aa, Wu:2009aa} but still generates original research \citep{Hofwing:2011aa,Cetisli:2011aa}. The modern formulation of the use of \ac{GA} to find polynomial models is known as \acf{EPR} and systematization can be traced back to the work of Davidson, Savic and Walters (\cite{Davidson:2003aa}). Further developments include multi-objective optimizations (\cite{Giustolisi:2009aa}). 

This paper describes an extension of the general \ac{EPR} method to find a regularized polynomial regression of a given dataset. The optimal regression results from a cost function that accounts for both the \ac{rmse} and a regularization factor to avoid overfitting. \note{Explain a bit more the overfitting avoiding method.}

%A method that produces \emph{adequate} models from observed complex data has many uses. For example by a scientist to better understand the source of the data or by an autonomous agents adapting to the environment.  

% It turns out that, discarding the computational cost of training, the regression method presented here, \acf{EPRR}, proves to be quite effective. Indeed, according to the results of common  methods on several real-world datasets it is only systematically out-performed by random forests, an \emph{ensemble} method.

The next section describes the method's details and is followed by a presentation of some performance results. The last section draws some conclusions and points future research tasks.

\section{Genetic Algorithms for Polynomials}

This section starts with a brief introduction and outline of the evolutionary polynomial regression algorithm, \ac{EPR}, and proceeds into core details as the encoding used to represent individual polynomial instances in the \ac{GA} populations and the regularization of the cost function.

An usual representation of polynomials is through expressions of the form
$$
p\left( x_1, \ldots, x_m\right) = \sum_i \theta_i q_i
$$
where each $q_i = \prod_{j} x_j^{\alpha_{ij}}$ is a monomial, the exponents $\alpha_{ij}\in\mathbb{N}_0$ are non-negative integers and the coefficients $\theta_i \in \mathbb{R}$ are real valued. For example $p\left( x_1, x_2, x_3 \right) = 2 x_1 + x_2 x_3 + \frac{1}{2} x_1^2 x_3$ has monomials $q_1 = x_1, q_2 = x_2 x_3$ and $q_3 = x_1^2 x_3$, exponents $\alpha_{1,1} = 1, \alpha_{2,2}=1, \alpha_{2,3} = 1, \alpha_{3,1} = 2, \alpha_{3,3} = 1$ and all other $\alpha_{ij} = 0$ and coefficients $\theta_1 = 2, \theta_2 = 1$ and $\theta_3 = 1/2$.

The exponents alone can be organized into a matrix $\left[\alpha_{ij}\right]$ that defines the monomial structure of the polynomial. For the example above the matrix representation of the monomials is
$$
\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 1 \\
2 & 0 & 1
\end{array}
\right] \sim 
\left[ 
\begin{array}{ccc}
x_1 \\
x_2 x_3 \\
x_1^2 x_3
\end{array}
\right]
$$
where each row defines a monomial and each column represents a variable. Changing the order of the rows doesn't change the polynomial whereas changing the order of the columns corresponds to changing the respective variables.
%Notice that if we interpret matrix $A$ as a single column of monomials and set the coefficients $T = \left[ \begin{array}{ccc} 2 & 1 & \frac{1}{2} \end{array} \right ]$ then
%$$
%p\left( x_1, x_2, x_3 \right) = TA = \left[ \begin{array}{ccc} 2 & 1 & \frac{1}{2} \end{array} \right ]\left[ 
%\begin{array}{ccc}
%x_1 \\
%x_2 x_3 \\
%x_1^2 x_3
%\end{array}
%\right].
%$$

This partial representation of polynomials makes the problem of structure search very clear: except for the trivial cases, the number of possible monomials given $n$ variables and a maximum joint degree $d$ grows exponentially with either $n$ or $d$. But more importantly, by separating the set of monomials from the coefficients, the polynomial regression problem can be naturally split into two subproblems:
%
\begin{enumerate}
\item For a given set of monomials $\mathcal{Q} = \left\lbrace q_1, \ldots, q_k\right\rbrace$ find the regression coefficients $\Theta = \left\lbrace \theta_1,\ldots,\theta_k \right\rbrace$ that minimize the \ac{rmse} on a given dataset;

\item Find the fittest set of monomials, \emph{i.e.} the polynomial that minimizes the \ac{rmse} on the same dataset;
\end{enumerate}
%
More precisely, concerning the first problem, let $\mathcal{D}$ be a dataset with $n$ observations of variables $Y, X_1,\ldots,X_m$ and $\mathcal{Q} = \left\lbrace q_1,\ldots, q_k\right\rbrace$ a set of $k$ monomial expressions over $X_1,\ldots,X_m$. Define the hypothesis\footnote{The expression ``$q|_{X=x}$'' reads ``\emph{$q$ with all instances of $X$ replaced by $x$}.''}
%
\begin{eqnarray*}
h_{\Theta,\mathcal{Q}}\left(x_1,\ldots,x_m\right) &=& \sum_{j = 1}^k \theta_j q_j|_{X_i=x_i,\forall 1 \leq i \leq m}
\end{eqnarray*}
%
and let the error (as ``cost'') be
%
\begin{eqnarray}
J_{\textrm{fit}}\left(\Theta;\mathcal{Q},\mathcal{D}\right)  & =  \nonumber \\
\sqrt{\frac{1}{n}\sum_{i=1}^n \left( y\sample{i} - h_{\Theta,\mathcal{Q}}\left( x_1\sample{i},\ldots,x_m\sample{i} \right) \right)^2 }\label{eq:rmse}
\end{eqnarray}
%
the usual \acf{rmse} function. Now the first problem can be stated as:
%
\emph{Given a dataset $\mathcal{D}$ and a set of monomials $\mathcal{Q}$ find parameters $\Theta$ that minimize the cost $J_{\textrm{fit}}\left(\Theta;\mathcal{Q},\mathcal{D}\right)$.}

%
This is a simple linear regression problem obtained by expanding $\mathcal{D}$ with columns that replicate the monomials in $\mathcal{Q}$. The resulting dataset, $\mathcal{D} \cup \mathcal{Q}\left( \mathcal{D} \right)$, adds the monomial transformations in $\mathcal{Q}$ to the original dataset $\mathcal{D}$. An alternative formulation would just replace $\mathcal{D}$ by $\mathcal{Q}\left( \mathcal{D} \right)$. It turns out that the first formulation is a special case of the second (by including the variables in the monomial set) and has better \ac{rmse} performance --- what is not surprising because it uses more features.

The second problem is treated in the \ac{GA} setting: Let $\mathcal{D}$ be a dataset as above and $\mathcal{P}$ a set of polynomials. For each polynomial $p\in \mathcal{P}$ let $\mathcal{Q}_p$ be the set of monomials in $p$ (without the coefficients) and define the (anti) fitness
%
\begin{eqnarray*}
\phi_p &=& \min_\Theta J_{\textrm{fit}}\left(\Theta;\mathcal{Q}_p,\mathcal{D}\right)
\end{eqnarray*}
%
by solving the first problem. With a fitness of every instance, the \ac{GA} genetic operators (usually mutation and crossover) evolve the population $\mathcal{P}$ until a reasonable approximation of a local minimum is found. 
%
\begin{algorithm}[t]
\begin{algorithmic}
\Function{EPR}{$D,pop_0,\epsilon, maxiter$}
	\State $pop \gets pop_0;\: err \gets 1.0+\epsilon$
	\While{$err > \epsilon \land iterations < maxiter$}
		\State $pop \gets \Call{IterateGA}{pop}$
		\State $pop \gets \Call{Sort}{pop, key = J}$\Comment{Sort population by regression error}
		\State $err \gets J\left(\Call{First}{pop}\right)$
	\EndWhile
	\State\Return $\Call{First}{pop}$
\EndFunction
\end{algorithmic}
\caption{This \ac{EPR} algorithm uses linear regression for the calculation of the \ac{rmse} $J$ and the space of polynomials is searched in the \acp{GA} iteration step. At exit the \ac{rmse} of the fittest instance is bounded by $\epsilon$ or the maximum number of allowed iterations.}\label{alg:gapoly}
\end{algorithm} 
%
Notice that the properties of \acp{GA} and linear regression entail that Algorithm \ref{alg:gapoly} converges to a polynomial that is a local minimum of the fitness function, encapsulated in the \ac{rmse} function $J_{\textrm{fit}}$.
%

%
Subsection \ref{subs:polynomial.encoding} describes the encoding of individual polynomial instances as chromosomes and other parameters used in the \ac{GA} implementation. The regularization of the cost function is discussed in subsection \ref{subs:cost.function}.

%
\subsection{Polynomial Encoding}\label{subs:polynomial.encoding}
%
%\begin{quotation}
%\note{Next paragraph moved here from the introduction.}
%Another important change is in the coding mechanism which introduces two new features: (a) inclusion of the linear terms in the polynomial expression, and (b) the activation/deactivation of monomials during evolution to allow the increase/decrease of polynomial expressions.
%\end{quotation}
%
The specific encoding (representation) of a set of monomials is an important aspect in the implementation of \ac{EPR}. The choice described below permits active and inactive monomials for regression purposes. The active (or inactive) state of a monomial might change through mutation or crossover. This simple mechanism enhances variation in the complexity of polynomial expressions by evolutionary operations.

Let  $\left\lbrace q_1, \ldots, q_k\right\rbrace$ be a set of monomials over the variables $X_1, \ldots, X_m$. The encoding of that set using $d$ bits per exponent is a binary list such that
\begin{enumerate}
\item the initial segment of $k$ bits defines the active state of each monomial;
\item the remaining bits are split into $k$ segments of size $m\times d$, each representing a monomial;
\item the bits in each monomial segment are split into $m$ sub-segments of size $d$. The $j\nth$ sub-segment is the binary representation of the degree of the variable $X_j$ in the enclosing monomial segment;
\end{enumerate}
This encoding can also be viewed as the flattening of the binary exponents in the matrix representation prefixed by the activation segment. The set $\left\lbrace x_1^3 x_3, x_3^7, x_1 x_2\right\rbrace$ (with $m = 3$) has matrix representation
$$\left[ 
\begin{array}{ccc}
3 & 0 & 1 \\
0 & 0 & 7 \\
1 & 1 & 0
\end{array}
\right] = \left[ 
\begin{array}{ccc}
011 & 000 & 001 \\
000 & 000 & 111 \\
001 & 001 & 000
\end{array}
\right]_{\left( 2 \right)}
$$
where the right matrix is in binary form using $d = 3$ bits. An encoding of this set of monomials with the extra monomial $x_1^6x_2^2x_3^5$ inactive, setting $k = 4$, would be
\begin{center}
1110;~011, 000, 001;~000,000,111;~001,001,000;~110,010,101
\end{center}
where, for reading purposes, semicolons separate segments and commas separate variables. The first $k=4$ bits inform that the first, second and third monomials are active while the fourth is not.

While each valid encoding represents a set of monomials the map is not bijective: each set of monomials has multiple encodings, for example by changing $d$ or the order of monomial segments. However, considering the \ac{EPR} task, this is a minor issue and a bijective map would add computational complexity and negative impact to the algorithm's performance.

There is one final remark concerning this encoding method. As it is, the activation segment can become all zeros, representing the empty set of monomials. This situation can be avoided with a simple hack: Given an encoding, the first monomial is always considered active, thus restricting the syntactic form of encodings to binary strings starting with $1$. In practice, this means that the implementation of the encoding can omit the first bit.
% If the coding consists of entirely zeros, by convention, it describes polynomial $x_1$. This has to do with the execution of the polynomial regression that would fail if we interpret it as the zero polynomial. Anyway, for progressive larger  binary descriptions, the chances of getting this zero description decrease exponentially, so it does not impact in any meaningful way in the algorithm's performance.

\subsection{Cost Function}\label{subs:cost.function}

The polynomial regression \ac{rmse} considered so far accounts for the ability to predict the transformed test-set. A known problem of using a cost function based only in the dataset \ac{rmse} (and of polynomial regressions in general) is the tendency to overfit  training data. This excessive variance can be reduced by regularizing the \ac{rmse} function and one possible method is the inclusion of a penalty factor proportional to the number of monomials. In this line the error from equation \ref{eq:rmse} defines a regularized version
\begin{eqnarray}
J_{\textrm{reg}}\left(\Theta, \lambda;\mathcal{Q},\mathcal{D}\right) &=& \lambda^{k} J_{\textrm{fit}}\left(\Theta;\mathcal{Q},\mathcal{D}\right)\label{eq:rmse-reg}
\end{eqnarray}
%
where $k$ is the number of monomials in the polynomial. When $\lambda > 1$ polynomials with more monomials are penalized. The regularized extension of \ac{EPR}, denoted by \ac{EPRR}, includes the regularized cost function. 

\begin{figure*}[tb]\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{figure_1a.pdf}
%
% RIGHT PANE
&
%
%
\includegraphics[width=0.49\textwidth]{figure_1b.pdf}
\end{tabular}

\caption{Error distribution by regularization exponent for two common datasets. The box plots summarize error values of ten simulations for each value of $\lambda$. The smallest overall error, in grey, is achieved in both datasets when $\lambda=0.8$. Performance of the non-regularized \ac{EPR} is plotted in the line $\lambda = 1$.}
\label{fig:lambda.error-distribution}\label{Abalone_dataset_lambdas}\label{Auto-MPG_dataset_lambdas}

\end{center}\end{figure*}

A simple exploration on the effect of the regularization parameter is depicted in Figure~\ref{fig:lambda.error-distribution} where it is possible to observe that the typical inflection point lies around $\lambda = 0.8$. This value, favoring ``larger'' polynomials is justified by the balance of the data's non-linearity and polynomial complexity: below $\lambda = 0.8$, even penalized, larger monomial sets achieve better \ac{rmse} performance than smaller ones while above that value the size of the monomial set is excessive. Within this tension the overall error results reduced when compared to the non-regularized \ac{EPR} version.

\subsection{Genetic Operators}

To perform the genetic algorithm it was used the R package genalg~\citep{Willighagen:2012aa}. The operators were the standard ones: (a)~crossover, \emph{i.e.}, a pair of solutions from the previous generations are combined by splitting and mixing their respective representations, and (b)~mutation, changing the values of single bits; the mutation chance applied in the datasets was 5\%. There was also elitism between generations, \emph{i.e.}, 20\% of the best solutions survive to the next generation.

\section{Experimental Results}

The results were found using \texttt{R} programming language~\citep{R-Core-Team:2013aa}\footnote{The datasets and the \texttt{R} code used to produce the results and plots in this paper are available online at \url{https://github.com/jpneto/GenAlgPoly}.}. To compare this paper's proposed algorithm we applied the exact same train and test samples using several well-known learning algorithms for regression, namely: the classic \ac{EPR}, Linear Regression, Support Vector Machines~\citep{Meyer:2012aa}, Regression Trees~\citep{Therneau:2013aa} and Conditional Inference Trees~\citep{Hothorn:2006aa,Strobl:2007aa,Strobl:2008aa}. To achieve better error results, the \ac{SVM} and Regression Tree algorithms had their parameters tuned.

In order to train and test the performance of \ac{EPRR} several mainstream datasets were used. For each dataset, we selected 70\% for training purposes and the remaining observations to make a test set in order to compute the estimated \ac{rmse}. To achieve more robust results, each dataset were processed $25$~times, each one with different samples for the train and test sets. For the datasets with attribute values of different magnitudes, a preliminary scaling was executed. The results below are box plots for the test set error predictions over these different runs.

\begin{figure*}[tb]\begin{center}
\includegraphics[width=0.98\textwidth]{figure_2.pdf}
\note{Plots of ``Regression Trees'' and ``Conditional Inference Trees'' look the same!}
\caption{Results for Artificial dataset. As shown, polynomial regression finds the exact polynomial structure that generates the dataset, reducing the test set prediction error to zero.  The regression methods depicted are: 1. \ac{EPRR}, 2. Linear Regression, 3. SVM, 4. Regression Trees and 5. Conditional Inference Trees}
\label{artificial_dataset1_lambda1.0}
\end{center}\end{figure*}

\begin{description}
\item{\textsc{Artificial}} this is an artificial dataset with four numeric features, $x_1, \ldots x_4$, where $x_1,x_3$ are outcomes from Poisson random variables, and $x_2,x_4$ from Normal random variables. The dependent variable~$y$ is given by expression $x_2x_4^2 + x_1^2x_3 + 5$. The dataset includes $n=50$~observations.

This dataset was used in order to verify if \ac{EPR} was able to find the polynomial relation, which the algorithm did (cf. figure~\ref{artificial_dataset1_lambda1.0}). The genetic algorithm run with a population of $n=100$~solutions with $50$~iterations for each run.

In the next datasets, the population of the genetic algorithm had size~$n=300$ with 100~iterations for each run. \ac{EPRR} was run with regularization parameter $lambda = 0.8$.

\begin{figure*}[tb]\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{figure_3a.pdf}
%
&
%
\includegraphics[width=0.48\textwidth]{figure_3b.pdf}
%
\\
%
\includegraphics[width=0.48\textwidth]{figure_3c.pdf}
%
&
%
\includegraphics[width=0.48\textwidth]{figure_3d.pdf}
%
\end{tabular}

\caption{Summary results for different regression methods on the Housing, Abalone, Auto MPG and Kinematics. Although \ac{EPRR} not always achieves the smallest error, performance is on-par with more sophisticated methods.  The regression methods depicted in these figures are: 1. \ac{EPRR}, 2. \ac{EPR}, 3. Linear Regression, 4. SVM, 5. Regression Trees and 6. Conditional Inference Trees}
\label{Housing_dataset_lambda0.8_25runs}
\label{Abalone_dataset_lambda0.8_25runs}
\label{Auto-Mpg_dataset_lambda0.8_25runs}
\label{Kinematics300_lambda0.8_25runs}
\end{center}\end{figure*}

\item{\textsc{Housing}}: This data set concerns the task of predicting housing values in areas of Boston. There are $m=13$ continuous attributes and the dependent variable is the median value of owner-occupied homes in \$1000's. There are $n=506$ observations.


\item{\textsc{Abalone}} This dataset can be used to predict the age of a abalone shell using the given $m=8$ numeric attributes concerning several physical measurements. There are $n=4177$ observations. 

% best polynomial, rsme 0.65
% $$y = 0.52 s_1^2 x_5 - 0.13 x_2 x_8 -0.75 x_1^2 x_6 + 0.78 x_8 + 0.18 $$

\item{\textsc{Auto MPG}} This dataset is used to predict fuel consumption in miles per gallon, based on two discrete and five continuous attributes ($m=7$). There are $n=398$ observations.

% best polynomial, \ac{rmse} 0.32
%$$ y = 0.4 x_6 + 0.02 x_5^3 x_6 - 0.85 x_4 + 0.23 x_4^2 - 0.26 $$

\item{\textsc{Kinematics}} This dataset is concerned with the realistic simulation of the forward kinematics of an 8 link robot arm. The task is to predict the distance of the end-effector from a target using $m=8$ continuous attributes. There are $n=8192$ observations. 

% best polynomial, \ac{rmse} 0.74
% $$ y = 0.24 x_5 - 0.23 x_5 x_6 x_7 - 0.19 x_4 x_7 - 0.5 x_3 $$

\end{description}

\subsection{Convergence speed}

The \ac{GA} quickly proceeds in the first~50 to~100 generations to reasonable error rates. Then, it proceeds slower achieving best solutions with marginal error reduction. Since the entire learning process takes some time, in the current R implementation, placing a limit between 50 to 100 generations already achieves good results, relative to higher iteration values. Figure~\ref{Abalone_fitnessProgress} shows a typical error evolution for the dataset Abalone given a regularized version ($\lambda = 0.975$) and a non-regularized ($\lambda = 1.0$).

\begin{figure*}[tb]
\begin{center}
\includegraphics[width=0.98\textwidth]{figure_4.pdf}
\caption{Error progress for Abalone dataset during a single execution of the genetic algorithm. The figure shows the fitness evolution for two different regularization values. The population for both consisted of $200$ polynomials. The error values seem to stabilize around iteration~250.}
\label{Abalone_fitnessProgress}

\end{center}
\end{figure*}

\section{Conclusion and Future Work}

\note{Discutir a localidade do regressor}

% Of all the regression methods considered, the proposed method is the one that shows, with one exception, the lowest error. Only Random Forest outperforms \ac{EPR} systematically (it also outperforms all the other regression algorithms). The single non-ensemble exception --- besides the artificial dataset that uses a straightforward polynomial relation --- is the Auto MPG dataset where \ac{EPR} has comparable results. This is evidence that applying standard genetic operators for polynomial model searching is a viable tool for regression purposes.

Of the regression methods considered, \ac{SVM} achieved the best results in 3 out of 4 datasets. However, \ac{SVM} was tuned for each particular dataset while \ac{EPRR} was executed with its parameters preset. Even so, \ac{EPRR} had the best error in the Abalone dataset, and achieved competitive results in the Housing and Auto MPG datasets. 

Comparing \ac{EPR} and \ac{EPRR} -- which is the main article's topic -- the regularized version achieved much better results at Abalone and especially Kinematics. The Housing improved errors when compared with \ac{EPR} in a difference in means, resulted in a 95\% HDI (Highest Density Interval) equal to [0.001, 0.119] which, while borderline, achieves statistical significance. Only in the Auto MOG dataset \ac{EPR} achieved better results, even if not that different from \ac{EPRR}.

For complexity considerations \ac{EPR} and \ac{EPRR} demands some processing time. On a quad-core computer, processing the Kinematics dataset (with 8k~observations) takes approximately 5~minutes. The processing time can probably be speeded between one to two orders in magnitude if the process is implemented in a low level programming language like \texttt{C++}. However, speed optimization was not the focus of this article.

A cross-validation procedure can be implemented to refine the appropriate parameter values to achieve better errors. Namely, the regularization parameter, $\lambda$, can be tested with several values, instead of being fixed at $0.8$. Other parameters like mutation chance or the amount of elitism could also be tested. However, these type of tests need a low-level, fast implementation of \ac{EPR} and were not considered.

\section*{Acknowledgements}

The authors are grateful to the Fundação para a Ciência e Tecnologia (FCT) and the  R\&D laboratory LabMAg for the financial support given to this work, under the strategic project \textsc{PEst-OE/EEI/UI0434/2011}.

Datasets used herein were selected from Luís Torgo's data repository, \url{http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}. Most can also be found in the UCI ML repository at \url{http://archive.ics.uci.edu/ml/}.

The authors wish to thank professor André Falcão for motivation and useful discussions around the article.

\section*{Bibliography}

\bibliographystyle{model2-names}
\bibliography{fullbib2}
    
\end{document}

